Using cuda device
/users/eleves-a/2023/heyuan.liu/.local/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.
  warnings.warn(
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
/users/eleves-a/2023/heyuan.liu/.local/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(terminated, (bool, np.bool8)):
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 1059     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 26.9        |
|    ep_rew_mean          | 26.9        |
| time/                   |             |
|    fps                  | 825         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008796453 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.686      |
|    explained_variance   | -0.00293    |
|    learning_rate        | 0.0003      |
|    loss                 | 6.63        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 50.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.6        |
|    ep_rew_mean          | 36.6        |
| time/                   |             |
|    fps                  | 816         |
|    iterations           | 3           |
|    time_elapsed         | 7           |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.009686211 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.0933      |
|    learning_rate        | 0.0003      |
|    loss                 | 15.1        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 38.9        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 47          |
|    ep_rew_mean          | 47          |
| time/                   |             |
|    fps                  | 814         |
|    iterations           | 4           |
|    time_elapsed         | 10          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.007370588 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.631      |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0003      |
|    loss                 | 21.1        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 47.8        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 60.7         |
|    ep_rew_mean          | 60.7         |
| time/                   |              |
|    fps                  | 812          |
|    iterations           | 5            |
|    time_elapsed         | 12           |
|    total_timesteps      | 10240        |
| train/                  |              |
|    approx_kl            | 0.0062203845 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.608       |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0003       |
|    loss                 | 21           |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0175      |
|    value_loss           | 56.4         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 75.1        |
|    ep_rew_mean          | 75.1        |
| time/                   |             |
|    fps                  | 811         |
|    iterations           | 6           |
|    time_elapsed         | 15          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012802638 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.0003      |
|    loss                 | 16.9        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 48.4        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 90.9        |
|    ep_rew_mean          | 90.9        |
| time/                   |             |
|    fps                  | 811         |
|    iterations           | 7           |
|    time_elapsed         | 17          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.010359306 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.579      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 16.7        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 57.6        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 107         |
| time/                   |             |
|    fps                  | 807         |
|    iterations           | 8           |
|    time_elapsed         | 20          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010704037 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.4        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 48          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | 127         |
| time/                   |             |
|    fps                  | 800         |
|    iterations           | 9           |
|    time_elapsed         | 23          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.008988813 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.8        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00793    |
|    value_loss           | 37.4        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 144          |
|    ep_rew_mean          | 144          |
| time/                   |              |
|    fps                  | 796          |
|    iterations           | 10           |
|    time_elapsed         | 25           |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0035866918 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.96         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00666     |
|    value_loss           | 23.6         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | 166         |
| time/                   |             |
|    fps                  | 792         |
|    iterations           | 11          |
|    time_elapsed         | 28          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.008756052 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 14          |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0086     |
|    value_loss           | 27.7        |
-----------------------------------------
/users/eleves-a/2023/heyuan.liu/.local/lib/python3.9/site-packages/gym/wrappers/record_video.py:75: UserWarning: [33mWARN: Overwriting existing videos at /users/eleves-a/2023/heyuan.liu/DeepReinforcementLearningCources/lab2/figs/lab2_cartpole_sb3_ppo_basic folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(

Moviepy - Building video /users/eleves-a/2023/heyuan.liu/DeepReinforcementLearningCources/lab2/figs/lab2_cartpole_sb3_ppo_basic/rl-video-episode-0.mp4.
Moviepy - Writing video /users/eleves-a/2023/heyuan.liu/DeepReinforcementLearningCources/lab2/figs/lab2_cartpole_sb3_ppo_basic/rl-video-episode-0.mp4

Using cuda device
/users/eleves-a/2023/heyuan.liu/.local/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.
  warnings.warn(
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
/users/eleves-a/2023/heyuan.liu/.local/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(terminated, (bool, np.bool8)):
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 20.6     |
| time/              |          |
|    fps             | 1086     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.7        |
|    ep_rew_mean          | 27.7        |
| time/                   |             |
|    fps                  | 879         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009097991 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.685      |
|    explained_variance   | -0.00264    |
|    learning_rate        | 0.0003      |
|    loss                 | 5.49        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 47.8        |
-----------------------------------------
/users/eleves-a/2023/heyuan.liu/.local/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=288.20 +/- 173.13
Episode length: 288.20 +/- 173.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008281414 |
|    clip_fraction        | 0.0446      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.0633      |
|    learning_rate        | 0.0003      |
|    loss                 | 15          |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 39          |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 36.7     |
| time/              |          |
|    fps             | 750      |
|    iterations      | 3        |
|    time_elapsed    | 8        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 51.2        |
|    ep_rew_mean          | 51.2        |
| time/                   |             |
|    fps                  | 747         |
|    iterations           | 4           |
|    time_elapsed         | 10          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.007454873 |
|    clip_fraction        | 0.0789      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0003      |
|    loss                 | 24.9        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 56          |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=153.00 +/- 21.38
Episode length: 153.00 +/- 21.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 153          |
|    mean_reward          | 153          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0077729104 |
|    clip_fraction        | 0.0565       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.617       |
|    explained_variance   | 0.225        |
|    learning_rate        | 0.0003       |
|    loss                 | 38           |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0151      |
|    value_loss           | 66.2         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65       |
|    ep_rew_mean     | 65       |
| time/              |          |
|    fps             | 724      |
|    iterations      | 5        |
|    time_elapsed    | 14       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 79.7       |
|    ep_rew_mean          | 79.7       |
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 6          |
|    time_elapsed         | 16         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.00468993 |
|    clip_fraction        | 0.0326     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.598     |
|    explained_variance   | 0.308      |
|    learning_rate        | 0.0003     |
|    loss                 | 33.1       |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.00971   |
|    value_loss           | 69         |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 94.7         |
|    ep_rew_mean          | 94.7         |
| time/                   |              |
|    fps                  | 727          |
|    iterations           | 7            |
|    time_elapsed         | 19           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0064925607 |
|    clip_fraction        | 0.0581       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.584       |
|    explained_variance   | 0.595        |
|    learning_rate        | 0.0003       |
|    loss                 | 28.3         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0114      |
|    value_loss           | 55.6         |
------------------------------------------
Eval num_timesteps=15000, episode_reward=495.00 +/- 10.00
Episode length: 495.00 +/- 10.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 495         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.003361566 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 14.8        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00823    |
|    value_loss           | 37.2        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 689      |
|    iterations      | 8        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 128         |
| time/                   |             |
|    fps                  | 694         |
|    iterations           | 9           |
|    time_elapsed         | 26          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.005841598 |
|    clip_fraction        | 0.0621      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.4        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00779    |
|    value_loss           | 37.1        |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 500         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.005090519 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.5        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 31.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 668      |
|    iterations      | 10       |
|    time_elapsed    | 30       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | 166         |
| time/                   |             |
|    fps                  | 674         |
|    iterations           | 11          |
|    time_elapsed         | 33          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.006755863 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.548      |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.84        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00459    |
|    value_loss           | 51.7        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | 182          |
| time/                   |              |
|    fps                  | 678          |
|    iterations           | 12           |
|    time_elapsed         | 36           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0054304195 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.553       |
|    explained_variance   | 0.929        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.72         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00408     |
|    value_loss           | 13.2         |
------------------------------------------
Eval num_timesteps=25000, episode_reward=500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 500         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.002359745 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.546      |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.005      |
|    value_loss           | 55.8        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 660      |
|    iterations      | 13       |
|    time_elapsed    | 40       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 218          |
|    ep_rew_mean          | 218          |
| time/                   |              |
|    fps                  | 665          |
|    iterations           | 14           |
|    time_elapsed         | 43           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0029023844 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.546       |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.16         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00377     |
|    value_loss           | 8.92         |
------------------------------------------
Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 500          |
|    mean_reward          | 500          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0052971155 |
|    clip_fraction        | 0.0604       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.531       |
|    explained_variance   | 0.574        |
|    learning_rate        | 0.0003       |
|    loss                 | 15.4         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0083      |
|    value_loss           | 47.3         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 235      |
| time/              |          |
|    fps             | 651      |
|    iterations      | 15       |
|    time_elapsed    | 47       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 251        |
|    ep_rew_mean          | 251        |
| time/                   |            |
|    fps                  | 656        |
|    iterations           | 16         |
|    time_elapsed         | 49         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.00917175 |
|    clip_fraction        | 0.0374     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.518     |
|    explained_variance   | 0.498      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.584      |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.00115   |
|    value_loss           | 15.7       |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 267          |
|    ep_rew_mean          | 267          |
| time/                   |              |
|    fps                  | 661          |
|    iterations           | 17           |
|    time_elapsed         | 52           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0077837557 |
|    clip_fraction        | 0.0916       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.499       |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0607       |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00762     |
|    value_loss           | 1            |
------------------------------------------
Eval num_timesteps=35000, episode_reward=500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 500         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.011329019 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.479      |
|    explained_variance   | 0.0581      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0551      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 0.584       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 649      |
|    iterations      | 18       |
|    time_elapsed    | 56       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 302          |
|    ep_rew_mean          | 302          |
| time/                   |              |
|    fps                  | 652          |
|    iterations           | 19           |
|    time_elapsed         | 59           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0066447128 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | 0.453        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0337       |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 0.381        |
------------------------------------------
Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 500          |
|    mean_reward          | 500          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0021179295 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.49        |
|    explained_variance   | -0.0935      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00973      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000435    |
|    value_loss           | 0.228        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 642      |
|    iterations      | 20       |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 333          |
|    ep_rew_mean          | 333          |
| time/                   |              |
|    fps                  | 646          |
|    iterations           | 21           |
|    time_elapsed         | 66           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0032561847 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.495       |
|    explained_variance   | -0.0517      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0139       |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 0.144        |
------------------------------------------
Eval num_timesteps=45000, episode_reward=500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 500         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.005888104 |
|    clip_fraction        | 0.0411      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.474      |
|    explained_variance   | -0.00913    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0124      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0028     |
|    value_loss           | 0.0979      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 349      |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 638      |
|    iterations      | 22       |
|    time_elapsed    | 70       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 366          |
|    ep_rew_mean          | 366          |
| time/                   |              |
|    fps                  | 643          |
|    iterations           | 23           |
|    time_elapsed         | 73           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0070612375 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.488       |
|    explained_variance   | -0.0371      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0246       |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 0.0646       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 379          |
|    ep_rew_mean          | 379          |
| time/                   |              |
|    fps                  | 646          |
|    iterations           | 24           |
|    time_elapsed         | 75           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0005928951 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.477       |
|    explained_variance   | -1.06e-05    |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0158       |
|    n_updates            | 230          |
|    policy_gradient_loss | 0.000317     |
|    value_loss           | 0.0407       |
------------------------------------------
Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 500          |
|    mean_reward          | 500          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0010501108 |
|    clip_fraction        | 0.0063       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.474       |
|    explained_variance   | 0.0586       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00593      |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000281     |
|    value_loss           | 0.0247       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 396      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 641      |
|    iterations      | 25       |
|    time_elapsed    | 79       |
|    total_timesteps | 51200    |
---------------------------------
[34m[1mwandb[0m: [33mWARNING[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
